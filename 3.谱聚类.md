## 谱聚类

### 拉普拉斯矩阵

拉普拉斯矩阵的应用场景非常丰富， 包括Sectral Clustering、Graph Fourier Transformation及Graph Convolution的定义都用到图的拉普拉斯矩阵。

对于图$G=(V, E)$,其拉普拉斯矩阵的定义为$L=D-A$，其中$L$为拉普拉斯矩阵， $D$为顶点度矩阵，$A$是图的邻接矩阵。如下图所示：

![img](/Users/mingchao.chen/OneDrive/学习/笔记/picture/matrixforgraph.png)

但需要注意的是， 常见的拉普拉斯矩阵表示实际上有三种：
$$
\left\{
\begin{array}
\\
L=D-A\\
L_{sym}=D^{-\frac12}LD^{\frac12}\\
L_{rw}=D^{-1}L     
\end{array}
\right.
$$
容易推导的是，拉普拉斯矩阵有如下性质：

1. 对任意向量$f\in \mathbb{R}_n$, 满足：
   $$
   f^TLf=\frac12\sum_{i,j=1}^{n}a_{ij}(f_i-f_j)^2 \label{sc1}
   $$

2. $L$的特征值均为实数，这是因为拉普拉斯矩阵一定是实对称矩阵的特性决定的

3. 拉普拉斯矩阵是半正定的，且对应的n个实数特征值都大于等于0，即$0=\lambda_1≤\lambda_2≤...≤\lambda_n$， 且最小的特征值为0，这个由性质2很容易得出。

### 无向图切图方法

对于无向图$G$的切图，我们的目标是将图$G(V,E)$切成相互没有连接的k个子图，每个子图点的集合为：$A_1,A_2...A_k$，它们满足$A_i\cap A_j =\empty$, 且$A_1\cup 𝐴_2 \cup...\cup𝐴_𝑘=𝑉$。

对于任意两个子图的集合$A,B\in V$，定义$A$和$B$的切图权重为:
$$
W(A,B)=\sum_{i\in A,j\in B}a_{ij}
$$

对于$k$个子图点的集合，$\{A_1,A_2...A_k\}$，定义其切图权重为:
$$
cut(A_1,A_2...A_K)=\frac12\sum_{i=1}^kW(A_i, 	\bar A_i)
$$
我们需要找到一种切图方式，最大化子图内部的点权重(度)之和，并最小化子图间的点权重(度)和。但是这种极小化的切图存在一个问题，如下图：

![img](/Users/mingchao.chen/OneDrive/学习/笔记/picture/cutgraph.jpg)



我们选择一个权重最小的边缘的点，比如C和H之间进行cut，这样可以最小化$cut(A_1,A_2...A_K)$, 但是却不是最优的切图。



### 切图聚类

#### RatioCut

RatioCut切图为了避免最小切图，对每个切图，不光考虑最小化$cut(A_1,A_2...A_K)$,它还同时考虑最大化每个子图点的个数，即：
$$
RatioCut(A_1,A_2...A_K)=\sum_{i=1}^k \frac{W(A_i, 	\bar A_i)}{|A_i|}
$$
为了最小化这个$RatioCut$值， 根据式$\eqref{sc1}$，引入如下指示向量$h_j=[h_{1j},h_{2j}...h_{nj}]^T$,
$$
h_{ij}=
\left\{
\begin{array}
\\
0\\
\frac{1}{\sqrt{|A_j|}},v_i\in A_j
\end{array}
\right.
\\
(i=1,2,...n; j=1,2...k)
$$
构造矩阵$H_{n*k}$, k个指示向量作为矩阵的列,根据式$\eqref{sc1}$容易得到：
$$
\begin{aligned}\label{sc2}
\\ 
h_j^TLh_j&=\frac12\sum_{x,y=1}^{n}a_{xy}(h_{xj}-h_{yj})^2\\
&=\frac12(\sum_{x\in A_j,y\notin A_j}a_{xy}(\frac{1}{\sqrt{|A_j|}}-0)^2+\sum_{x\notin A_j,y\in A_j}a_{xy}(0-\frac{1}{\sqrt{|A_j|}})^2)\\

&=\frac12(\sum_{x\in A_j,y\notin A_j}a_{xy}\frac{1}{|A_j|}+\sum_{x\notin A_j,y\in A_j}a_{xy}\frac{1}{|A_j|})\\

&=\sum_{x\in A_j,y\notin A_j}a_{xy}\frac{1}{|A_j|}\\
&=\frac{W(A_j, \bar A_j)}{|A_j|}
\end{aligned}
$$
所以
$$
\begin{aligned}
RatioCut(A_1,A_2...A_K)&=\sum_{i=1}^k \frac{W(A_i, 	\bar A_i)}{|A_i|}
\\
&=\sum_{i=1}^k h_i^TLh_i\\
&=tr(H^TLH)
\end{aligned}
$$
其中$tr(H^TLH)$为矩阵的迹， 所以我们的目标函数为:
$$
\arg\min{tr(H^TLH)} \qquad \mathrm{ s.t. }H^TH=I
$$

$$
\because h_i^Th_i=1\\
\therefore 令 h_i^TLh_i=\lambda\\
\Rightarrow Lh_i=\lambda h_i
$$

$h$是单位正交基， $L$为对称矩阵，此时$h_i^TLh_i$的最大值为$L$的最大特征值，最小值是$L$的最小特征值。$h$是对应的特征向量。

也就是说， $H$的$k$列对应的是$L$前$k$个特征值对应的特征向量。只需要对得到的$H$做一个传统的聚类， 就可以得到节点所属的集合。

#### NCut切图

NCut切图和RatioCut切图很类似，但是把Ratiocut的分母$|A_i|$换成$vol(A_i)$。 $vol(A_i)$表示$A_i$中的节点度之和。一般来说NCut切图优于RatioCut切图。
$$
NCut(A_1,A_2...A_k)=\sum_{i=1}^k \frac{W(A_i, 	\bar A_i)}{vol(A_i)}
$$
NCut的指示向量为:
$$
h_{ij}=
\left\{
\begin{array}
\\
0\\
\frac{1}{vol(A_j)},v_i\in A_j
\end{array}
\right.
\\
(i=1,2,...n; j=1,2...k)
$$
同RatioCut推导式,
$$
\begin{aligned}\label{sc3}
\\ 
h_j^TLh_j&=\frac12\sum_{x,y=1}^{n}a_{xy}(h_{xj}-h_{yj})^2\\
&=\frac12(\sum_{x\in A_j,y\notin A_j}a_{xy}(\frac{1}{\sqrt{vol(A_j)}}-0)^2+\sum_{x\notin A_j,y\in A_j}a_{xy}(0-\frac{1}{\sqrt{vol(A_j)}})^2)\\

&=\frac12(\sum_{x\in A_j,y\notin A_j}a_{xy}\frac{1}{vol(A_j)}+\sum_{x\notin A_j,y\in A_j}a_{xy}\frac{1}{vol(A_j)})\\

&=\sum_{x\in A_j,y\notin A_j}a_{xy}\frac{1}{vol(A_j)}\\
&=\frac{W(A_j, \bar A_j)}{vol(A_j)}
\end{aligned}
$$

$$
\therefore
NCut(A_1,A_2...A_K)
=\sum_{i=1}^k \frac{W(A_i, 	\bar A_i)}{vol(A_i)}
=\sum_{i=1}^k h_i^TLh_i
=tr(H^TLH)
$$

但是和RatioCut不同的是， 此时$H^TH\not=I, H^TDH=I$,推导如下：
$$
h^T_jDh_j=\sum_{i=1}^nh_{ij}^2d_i=\frac{1}{vol(A_j)}\sum_{i\in A_j}d_i=1
\\ \Rightarrow H^TDH=I
$$
此时， 令$F=D^{\frac{1}{2}}H, H=D^{-\frac{1}{2}}F$,则$ F^TF=H^TD^{\frac{1}{2}}D^{\frac{1}{2}}H=I $, 而$H^TLH=F^TD^{-\frac{1}{2}}LD^{-\frac{1}{2}}F=F^TL_{sym}F$, 同RatioCut,只需要求出$L_{sym}$对应的特征值和特征向量即可。



#### Demo

##### python示例代码

```python
from sklearn.cluster import SpectralClustering
import torch
import networkx as nx
from sklearn.cluster import KMeans
import seaborn as sns

# 获得空手道俱乐部数据
G = nx.karate_club_graph()
real_tag = [0 if G.nodes[node]['club'] == 'Mr. Hi' else 1 for node in G.nodes]
nx.draw_networkx(G, node_color=real_tag)

# 添加自连接边
A = torch.FloatTensor(nx.adjacency_matrix(G).todense())
A = A + torch.eye(A.size(0))
d = A.sum(1)
L = torch.diag(d) - A


D = torch.diag(torch.pow(d, -0.5))
L_normed_L = D.mm(L).mm(D)

# ### 计算RatioCut
eigenvalue, featurevector = torch.eig(L, eigenvectors=True)
eigenvalue1 = eigenvalue[:, 0]
sns.scatterplot(x=range(34), y=eigenvalue1[eigenvalue1.argsort()], hue=real_tag)
features_list = featurevector[:, eigenvalue1.argsort()[1:3]]
std_list = torch.std(features_list, dim=1, keepdim=True)
mean_list = torch.mean(features_list, 1, keepdim=True)
H = (features_list-mean_list)/std_list

sp_kmeans = KMeans(n_clusters=2).fit(H)
sp_kmeans.labels_
nx.draw_networkx(G, node_color=sp_kmeans.labels_)


# ### 使用Ncut 切图

eigenvalue, featurevector = torch.eig(L_normed_L, eigenvectors=True)

eigenvalue1 = eigenvalue[:, 0]
sns.scatterplot(x=range(34), y=eigenvalue1[eigenvalue1.argsort()], hue=real_tag)
features_list = featurevector[:, eigenvalue1.argsort()[:2]]
sp_kmeans = KMeans(n_clusters=2).fit(features_list)
sp_kmeans.labels_
nx.draw_networkx(G, node_color=sp_kmeans.labels_)

# ### scikit的包
sc = SpectralClustering(2, affinity='precomputed', n_init=100).fit(A)
nx.draw_networkx(G, node_color=sc.labels_)
```

##### 真实值

![image-20210511114805992](/Users/mingchao.chen/OneDrive/学习/笔记/picture/image-20210511114805992.png)

##### RatioCut结果

![image-20210511114823755](/Users/mingchao.chen/OneDrive/学习/笔记/picture/image-20210511114823755.png)

##### NCut结果

![image-20210511114838614](/Users/mingchao.chen/OneDrive/学习/笔记/picture/image-20210511114838614.png)